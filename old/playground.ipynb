{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from backtesting import Backtest, Strategy\n",
    "from backtesting.lib import crossover\n",
    "import os\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN Network\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "# Define a simple stock trading environment\n",
    "class TradingEnv:\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "        self.initial_balance = 10000  # Define initial balance here\n",
    "        self.reset_stock()  # Call reset_stock after defining initial_balance\n",
    "\n",
    "    def reset_stock(self):\n",
    "        self.current_data = random.choice(self.data_list)\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance  # Now this works because initial_balance is defined\n",
    "        self.position = 0\n",
    "        self.total_value = self.initial_balance  # Reset total value\n",
    "        return self.get_state()\n",
    "\n",
    "    def reset(self):\n",
    "        return self.reset_stock()\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step >= len(self.current_data) - 1:\n",
    "            done = True\n",
    "            return self.get_state(), 0, done, {}\n",
    "\n",
    "        current_price = self.current_data['Close'][self.current_step]\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if action == 0:  # Buy\n",
    "            if self.balance >= current_price:\n",
    "                self.position += 1\n",
    "                self.balance -= current_price\n",
    "                reward = 1  # Incentivize buying\n",
    "\n",
    "        elif action == 1:  # Sell\n",
    "            if self.position > 0:\n",
    "                self.position -= 1\n",
    "                self.balance += current_price\n",
    "                reward = 1  # Incentivize selling\n",
    "\n",
    "        self.total_value = self.balance + self.position * current_price\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.current_data) - 1\n",
    "\n",
    "        return self.get_state(), reward, done, {}\n",
    "\n",
    "    def get_state(self):\n",
    "        return np.array([\n",
    "            self.balance,\n",
    "            self.position,\n",
    "            self.current_data['Open'][self.current_step],\n",
    "            self.current_data['High'][self.current_step],\n",
    "            self.current_data['Low'][self.current_step],\n",
    "            self.current_data['Close'][self.current_step]\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stock data from CSV files in a folder using *\n",
    "def get_stock_data_from_folder(folder_path):\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    data_list = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df = df[['Open', 'High', 'Low', 'Close', 'Volume']].dropna()\n",
    "        data_list.append(df)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNBacktestStrategy(Strategy):\n",
    "    def __init__(self, model, lookback=50):\n",
    "        self.model = model\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def init(self):\n",
    "        self.data_len = len(self.data)\n",
    "        self.last_trade = 0\n",
    "\n",
    "    def next(self):\n",
    "        # Your logic for the strategy\n",
    "        # For example, using the model to make predictions on the current state\n",
    "        state = self.data[-self.lookback:].to_numpy()  # Get the last `lookback` data points\n",
    "        action = self.model.predict(state)  # Use the model to predict the next action\n",
    "\n",
    "        if action == 0:  # Buy\n",
    "            if self.position == 0:\n",
    "                self.buy()\n",
    "        elif action == 1:  # Sell\n",
    "            if self.position > 0:\n",
    "                self.sell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNBacktestStrategy(Strategy):\n",
    "    def __init__(self, model, lookback=50):\n",
    "        self.model = model\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def init(self):\n",
    "        self.data_len = len(self.data)\n",
    "        self.last_trade = 0\n",
    "        \n",
    "    def next(self):\n",
    "        state = np.array([\n",
    "            self.balance,\n",
    "            self.position,\n",
    "            self.data.Open[-1],\n",
    "            self.data.High[-1],\n",
    "            self.data.Low[-1],\n",
    "            self.data.Close[-1]\n",
    "        ])\n",
    "        state = self.data[-self.lookback:].to_numpy()  # Get the last `lookback` data points\n",
    "        action = self.model.predict(state)  # Use the model to predict the next action\n",
    "\n",
    "        if action == 0:  # Buy\n",
    "            if self.position == 0:\n",
    "                self.buy()\n",
    "        elif action == 1:  # Sell\n",
    "            if self.position > 0:\n",
    "                self.sell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_dqn(data_list, model, target_model, episodes=100, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "    env = TradingEnv(data_list)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    replay_buffer = deque(maxlen=2000)\n",
    "    batch_size = 32\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = torch.FloatTensor(env.reset()).to(device)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice([0, 1])  # Random action\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    action = torch.argmax(model(state)).item()\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            reward = torch.tensor(reward, device=device, dtype=torch.float)\n",
    "            total_reward += reward.item()\n",
    "\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                states = torch.stack(states).to(device)\n",
    "                actions = torch.LongTensor(actions).to(device)\n",
    "                rewards = torch.stack(rewards).to(device)\n",
    "                next_states = torch.stack(next_states).to(device)\n",
    "                dones = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    q_next = target_model(next_states).max(1)[0]\n",
    "                    q_targets = rewards + gamma * q_next * (~dones)\n",
    "\n",
    "                q_values = model(states).gather(1, actions.view(-1, 1)).squeeze()\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, q_targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{episodes} - Total Reward: {total_reward}, Epsilon: {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save and load the model\n",
    "def save_model(model, path=\"dqn_trading_model.pth\"):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(path=\"dqn_trading_model.pth\", input_dim=6, action_dim=2):\n",
    "    model = DQNetwork(input_dim=input_dim, action_dim=action_dim)\n",
    "    model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100 - Total Reward: 3250.0, Epsilon: 0.995\n",
      "Episode 2/100 - Total Reward: 3154.0, Epsilon: 0.990025\n",
      "Episode 3/100 - Total Reward: 3240.0, Epsilon: 0.985074875\n",
      "Episode 4/100 - Total Reward: 2906.0, Epsilon: 0.9801495006250001\n",
      "Episode 5/100 - Total Reward: 3192.0, Epsilon: 0.9752487531218751\n",
      "Episode 6/100 - Total Reward: 2813.0, Epsilon: 0.9703725093562657\n",
      "Episode 7/100 - Total Reward: 5982.0, Epsilon: 0.9655206468094844\n",
      "Episode 8/100 - Total Reward: 459.0, Epsilon: 0.960693043575437\n",
      "Episode 9/100 - Total Reward: 1397.0, Epsilon: 0.9558895783575597\n",
      "Episode 10/100 - Total Reward: 2841.0, Epsilon: 0.9511101304657719\n",
      "Episode 11/100 - Total Reward: 3132.0, Epsilon: 0.946354579813443\n",
      "Episode 12/100 - Total Reward: 2883.0, Epsilon: 0.9416228069143757\n",
      "Episode 13/100 - Total Reward: 5874.0, Epsilon: 0.9369146928798039\n",
      "Episode 14/100 - Total Reward: 1277.0, Epsilon: 0.9322301194154049\n",
      "Episode 15/100 - Total Reward: 3087.0, Epsilon: 0.9275689688183278\n",
      "Episode 16/100 - Total Reward: 2703.0, Epsilon: 0.9229311239742362\n",
      "Episode 17/100 - Total Reward: 466.0, Epsilon: 0.918316468354365\n",
      "Episode 18/100 - Total Reward: 2785.0, Epsilon: 0.9137248860125932\n",
      "Episode 19/100 - Total Reward: 5705.0, Epsilon: 0.9091562615825302\n",
      "Episode 20/100 - Total Reward: 447.0, Epsilon: 0.9046104802746175\n",
      "Episode 21/100 - Total Reward: 438.0, Epsilon: 0.9000874278732445\n",
      "Episode 22/100 - Total Reward: 2653.0, Epsilon: 0.8955869907338783\n",
      "Episode 23/100 - Total Reward: 408.0, Epsilon: 0.8911090557802088\n",
      "Episode 24/100 - Total Reward: 1232.0, Epsilon: 0.8866535105013078\n",
      "Episode 25/100 - Total Reward: 3024.0, Epsilon: 0.8822202429488013\n",
      "Episode 26/100 - Total Reward: 419.0, Epsilon: 0.8778091417340573\n",
      "Episode 27/100 - Total Reward: 407.0, Epsilon: 0.8734200960253871\n",
      "Episode 28/100 - Total Reward: 1186.0, Epsilon: 0.8690529955452602\n",
      "Episode 29/100 - Total Reward: 2665.0, Epsilon: 0.8647077305675338\n",
      "Episode 30/100 - Total Reward: 2902.0, Epsilon: 0.8603841919146962\n",
      "Episode 31/100 - Total Reward: 2552.0, Epsilon: 0.8560822709551227\n",
      "Episode 32/100 - Total Reward: 448.0, Epsilon: 0.851801859600347\n",
      "Episode 33/100 - Total Reward: 1283.0, Epsilon: 0.8475428503023453\n",
      "Episode 34/100 - Total Reward: 2910.0, Epsilon: 0.8433051360508336\n",
      "Episode 35/100 - Total Reward: 1310.0, Epsilon: 0.8390886103705794\n",
      "Episode 36/100 - Total Reward: 1230.0, Epsilon: 0.8348931673187264\n",
      "Episode 37/100 - Total Reward: 1154.0, Epsilon: 0.8307187014821328\n",
      "Episode 38/100 - Total Reward: 1186.0, Epsilon: 0.8265651079747222\n",
      "Episode 39/100 - Total Reward: 441.0, Epsilon: 0.8224322824348486\n",
      "Episode 40/100 - Total Reward: 376.0, Epsilon: 0.8183201210226743\n",
      "Episode 41/100 - Total Reward: 399.0, Epsilon: 0.8142285204175609\n",
      "Episode 42/100 - Total Reward: 2719.0, Epsilon: 0.810157377815473\n",
      "Episode 43/100 - Total Reward: 2756.0, Epsilon: 0.8061065909263957\n",
      "Episode 44/100 - Total Reward: 2663.0, Epsilon: 0.8020760579717637\n",
      "Episode 45/100 - Total Reward: 2730.0, Epsilon: 0.798065677681905\n",
      "Episode 46/100 - Total Reward: 2678.0, Epsilon: 0.7940753492934954\n",
      "Episode 47/100 - Total Reward: 5031.0, Epsilon: 0.7901049725470279\n",
      "Episode 48/100 - Total Reward: 4961.0, Epsilon: 0.7861544476842928\n",
      "Episode 49/100 - Total Reward: 370.0, Epsilon: 0.7822236754458713\n",
      "Episode 50/100 - Total Reward: 4895.0, Epsilon: 0.778312557068642\n",
      "Episode 51/100 - Total Reward: 399.0, Epsilon: 0.7744209942832988\n",
      "Episode 52/100 - Total Reward: 2612.0, Epsilon: 0.7705488893118823\n",
      "Episode 53/100 - Total Reward: 1309.0, Epsilon: 0.7666961448653229\n",
      "Episode 54/100 - Total Reward: 4794.0, Epsilon: 0.7628626641409962\n",
      "Episode 55/100 - Total Reward: 4583.0, Epsilon: 0.7590483508202912\n",
      "Episode 56/100 - Total Reward: 2577.0, Epsilon: 0.7552531090661897\n",
      "Episode 57/100 - Total Reward: 4972.0, Epsilon: 0.7514768435208588\n",
      "Episode 58/100 - Total Reward: 2660.0, Epsilon: 0.7477194593032545\n",
      "Episode 59/100 - Total Reward: 1291.0, Epsilon: 0.7439808620067382\n",
      "Episode 60/100 - Total Reward: 340.0, Epsilon: 0.7402609576967045\n",
      "Episode 61/100 - Total Reward: 2506.0, Epsilon: 0.736559652908221\n",
      "Episode 62/100 - Total Reward: 2625.0, Epsilon: 0.7328768546436799\n",
      "Episode 63/100 - Total Reward: 2583.0, Epsilon: 0.7292124703704616\n",
      "Episode 64/100 - Total Reward: 2628.0, Epsilon: 0.7255664080186093\n",
      "Episode 65/100 - Total Reward: 2830.0, Epsilon: 0.7219385759785162\n",
      "Episode 66/100 - Total Reward: 4570.0, Epsilon: 0.7183288830986236\n",
      "Episode 67/100 - Total Reward: 398.0, Epsilon: 0.7147372386831305\n",
      "Episode 68/100 - Total Reward: 2573.0, Epsilon: 0.7111635524897149\n",
      "Episode 69/100 - Total Reward: 1311.0, Epsilon: 0.7076077347272662\n",
      "Episode 70/100 - Total Reward: 4674.0, Epsilon: 0.7040696960536299\n",
      "Episode 71/100 - Total Reward: 2426.0, Epsilon: 0.7005493475733617\n",
      "Episode 72/100 - Total Reward: 2719.0, Epsilon: 0.697046600835495\n",
      "Episode 73/100 - Total Reward: 2525.0, Epsilon: 0.6935613678313175\n",
      "Episode 74/100 - Total Reward: 383.0, Epsilon: 0.6900935609921609\n",
      "Episode 75/100 - Total Reward: 4852.0, Epsilon: 0.6866430931872001\n",
      "Episode 76/100 - Total Reward: 2477.0, Epsilon: 0.6832098777212641\n",
      "Episode 77/100 - Total Reward: 405.0, Epsilon: 0.6797938283326578\n",
      "Episode 78/100 - Total Reward: 1127.0, Epsilon: 0.6763948591909945\n",
      "Episode 79/100 - Total Reward: 295.0, Epsilon: 0.6730128848950395\n",
      "Episode 80/100 - Total Reward: 4544.0, Epsilon: 0.6696478204705644\n",
      "Episode 81/100 - Total Reward: 2368.0, Epsilon: 0.6662995813682115\n",
      "Episode 82/100 - Total Reward: 1167.0, Epsilon: 0.6629680834613705\n",
      "Episode 83/100 - Total Reward: 1000.0, Epsilon: 0.6596532430440636\n",
      "Episode 84/100 - Total Reward: 4516.0, Epsilon: 0.6563549768288433\n",
      "Episode 85/100 - Total Reward: 2307.0, Epsilon: 0.653073201944699\n",
      "Episode 86/100 - Total Reward: 4638.0, Epsilon: 0.6498078359349755\n",
      "Episode 87/100 - Total Reward: 362.0, Epsilon: 0.6465587967553006\n",
      "Episode 88/100 - Total Reward: 2813.0, Epsilon: 0.6433260027715241\n",
      "Episode 89/100 - Total Reward: 1160.0, Epsilon: 0.6401093727576664\n",
      "Episode 90/100 - Total Reward: 2187.0, Epsilon: 0.6369088258938781\n",
      "Episode 91/100 - Total Reward: 2212.0, Epsilon: 0.6337242817644086\n",
      "Episode 92/100 - Total Reward: 4959.0, Epsilon: 0.6305556603555866\n",
      "Episode 93/100 - Total Reward: 1413.0, Epsilon: 0.6274028820538087\n",
      "Episode 94/100 - Total Reward: 476.0, Epsilon: 0.6242658676435396\n",
      "Episode 95/100 - Total Reward: 2930.0, Epsilon: 0.6211445383053219\n",
      "Episode 96/100 - Total Reward: 2521.0, Epsilon: 0.6180388156137953\n",
      "Episode 97/100 - Total Reward: 2930.0, Epsilon: 0.6149486215357263\n",
      "Episode 98/100 - Total Reward: 1352.0, Epsilon: 0.6118738784280476\n",
      "Episode 99/100 - Total Reward: 2828.0, Epsilon: 0.6088145090359074\n",
      "Episode 100/100 - Total Reward: 468.0, Epsilon: 0.6057704364907278\n",
      "Model saved to dqn_trading_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the stock data and train the model\n",
    "folder_path = \"/home/artzuros/Documents/CS/Trader/trading/data/SCRIP/train\"\n",
    "data_list = get_stock_data_from_folder(folder_path)\n",
    "\n",
    "input_dim = 6\n",
    "action_dim = 2\n",
    "model = DQNetwork(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "target_model = DQNetwork(input_dim=input_dim, action_dim=action_dim).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "# Train the model (you can train for more episodes)\n",
    "train_dqn(data_list, model, target_model, episodes=100)\n",
    "\n",
    "# Save the trained model\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19717/3098006057.py:5: UserWarning: Data index is not datetime. Assuming simple periods, but `pd.DateTimeIndex` is advised.\n",
      "  backtest = Backtest(data, DQNBacktestStrategy, cash=10000, commission=0.002)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DQNBacktestStrategy.__init__() takes from 2 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Run the backtest with the DQN strategy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m backtest \u001b[38;5;241m=\u001b[39m Backtest(data, DQNBacktestStrategy, cash\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, commission\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.002\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mbacktest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Print results of the backtest\u001b[39;00m\n\u001b[1;32m      9\u001b[0m backtest\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[0;32m~/miniconda3/envs/trader/lib/python3.10/site-packages/backtesting/backtesting.py:1186\u001b[0m, in \u001b[0;36mBacktest.run\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m data \u001b[38;5;241m=\u001b[39m _Data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m   1185\u001b[0m broker: _Broker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broker(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m-> 1186\u001b[0m strategy: Strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbroker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1188\u001b[0m strategy\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m   1189\u001b[0m data\u001b[38;5;241m.\u001b[39m_update()  \u001b[38;5;66;03m# Strategy.init might have changed/added to data.df\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: DQNBacktestStrategy.__init__() takes from 2 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Get data for backtesting from a single stock (you can use any dataset)\n",
    "data = pd.read_csv('/home/artzuros/Documents/CS/Trader/trading/data/SCRIP/test/ADANIGREEN.csv')\n",
    "\n",
    "# Run the backtest with the DQN strategy\n",
    "backtest = Backtest(data, DQNBacktestStrategy, cash=10000, commission=0.002)\n",
    "backtest.run()\n",
    "\n",
    "\n",
    "# Print results of the backtest\n",
    "backtest.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
